{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observaciones convencionales horarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se extraen los datos climáticos horarios de las estaciones automáticas de la AEMET generados en las 24 h últimas. Se pueden descargar todas las estaciones o de las estaciones determinadas.\n",
    "\n",
    "Si se hiciera recurrentemente se "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Archivo donde está guardada la API key\n",
    "ruta_API = 'F:/Series/AEMET/apikey_AEMET.txt'\n",
    "# Carpeta en la que se guardarán las series\n",
    "ruta_series = 'F:/Series/AEMET/Datos/Horarios/'\n",
    "# Estaciones a importar: todas o una lista de las que se quiera\n",
    "codigos = ['todas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Estaciones automáticas\n",
    "\n",
    "Se importa el listado de las estaciones automáticas de la AEMET en toda España. Estas estaciones son las susceptibles de aportar datos por este medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº de estaciones automáticas de la AEMET:  852\n"
     ]
    }
   ],
   "source": [
    "estaciones =pd.read_csv('F:/Cartografia/AEMET/Estaciones_Automaticas.csv')\n",
    "estaciones.drop(['FID'], axis=1, inplace=True)\n",
    "estaciones.columns = ['idema', 'ubi', 'PROVINCIA', 'alt', 'COORD_X', 'COORD_Y', 'VAR_OBSERV', 'DATUM', 'TIPO']\n",
    "estaciones.sort_values('idema', inplace=True)\n",
    "estaciones.set_index('idema', drop=False, inplace=True)\n",
    "\n",
    "# Corregir caracteres conflictivos en el nombre de las estaciones\n",
    "for i in estaciones.index:\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('/', '-')\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('\"', '')\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('Á', 'A')\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('É', 'E')\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('Í', 'I')\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('Ó', 'O')\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('Ú', 'U')\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('À', 'A')\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('È', 'E')\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('Ç', 'C')\n",
    "    estaciones.loc[i, 'ubi'] = estaciones.loc[i, 'ubi'].replace('Ñ', 'N')\n",
    "\n",
    "print('Nº de estaciones automáticas de la AEMET: ', estaciones.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extraer todas las estaciones\n",
    "\n",
    "Se extraen los datos climáticos horarios disponibles de las últimas 24 h.\n",
    "\n",
    "### 1.1 Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hourly_all(API_path):\n",
    "    \"\"\"It extracts hourly climatic data from AEMET's API for all the stations available during the last 24 h.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    API_path: string. The path where the API key text file is saved, including the name and file extension.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extraer de AEMET los datos del último día\n",
    "    # -----------------------------------------\n",
    "     # Carga la api key \n",
    "    api_key = open(API_path).read().rstrip()\n",
    "    querystring = {\"api_key\": api_key}\n",
    "  \n",
    "    # Obtenemos información de todas las estaciones disponibles\n",
    "    url = \"https://opendata.aemet.es/opendata/api/observacion/convencional/todas\"\n",
    "    \n",
    "    iterate = True\n",
    "    \n",
    "    while iterate:\n",
    "        # Peticion de datos a la url indicada\n",
    "        r = requests.get(url, params=querystring, verify=False)\n",
    "        # Si no me deja hacer la conexión, la repito  \n",
    "        iterate = (r.status_code == requests.codes.too_many_requests)\n",
    "        print(r.json())\n",
    "        \n",
    "        # Chequeo si la petición ha ido bien    \n",
    "        if r.status_code == requests.codes.ok:\n",
    "            # Hago la petición para obtener los datos\n",
    "            data_url = r.json()['datos']\n",
    "            r_data = requests.get(data_url, params=querystring, verify=False)\n",
    "\n",
    "            # INCONSISTENCIA DE LA API:\n",
    "            # Cuando no encuentra datos en el rango seleccionado, la API devuelve\n",
    "            # que el status code es 200 (todo ok) y devuelve un json con el error\n",
    "            # cuando encuentra, no hay atributo estado            \n",
    "            try:\n",
    "                estado = r_data.json()['estado']\n",
    "            except:\n",
    "                estado = 200\n",
    "\n",
    "            # Si ha ido bien guardo los datos\n",
    "            if estado == requests.codes.ok:\n",
    "                #print(r_data.json())\n",
    "                data = (r_data.json())\n",
    "                # Crear una lista con cada una de las entradas de los datos brutos convertidos en data frames\n",
    "                ls = []\n",
    "                for i in np.arange(len(data)):\n",
    "                    ls.append(pd.DataFrame(data[i], index=[i]))\n",
    "                # Combinar los data frames de la lista en un unico data frame y definir la hora como índice\n",
    "                df = pd.concat(ls)\n",
    "                df.fint = pd.to_datetime(df.fint, yearfirst=True)\n",
    "                df.set_index('fint', inplace=True)\n",
    "                # Reorganizar algunas columnas\n",
    "                col = df.columns.tolist()\n",
    "                col.remove('alt')\n",
    "                col.remove('idema')\n",
    "                col.remove('lat')\n",
    "                col.remove('lon')\n",
    "                col.remove('ubi')\n",
    "                col = ['idema', 'ubi', 'lat', 'lon', 'alt'] + col\n",
    "                df = df[col]\n",
    "            else:\n",
    "                print(r_data.json()['descripcion'])\n",
    "        else:\n",
    "            print(r.json()['descripcion'])\n",
    "\n",
    "        time.sleep(60/45)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_serie_all(data, path):\n",
    "    \"\"\"It joins the past and present series of hourly climatic data. If there's not previous data, it simply creates a new\n",
    "    csv file with the hourly data from the last 24 h.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data:     data frame. The result of function 'extract_hourly_data'.\n",
    "    path:     string. File path where the previous data is saved (or where the new series will be created).\n",
    "    \"\"\"\n",
    "    # Nombre de las estaciones\n",
    "    estaciones = data.idema.unique()\n",
    "    \n",
    "    for i in range(len(estaciones)):\n",
    "        # Extraer las filas correspondientes a una estación\n",
    "        data_stn = data.loc[data.idema == estaciones[i], :]\n",
    "        cod = data_stn.idema[0]\n",
    "        name = data_stn.ubi[0]\n",
    "        # Corregir caracteres conflictivos en el nombre de las estaciones\n",
    "        name = name.replace('/', '-')\n",
    "        name = name.replace('\"', '')\n",
    "        name = name.replace('Á', 'A')\n",
    "        name = name.replace('É', 'E')\n",
    "        name = name.replace('Í', 'I')\n",
    "        name = name.replace('Ó', 'O')\n",
    "        name = name.replace('Ú', 'U')\n",
    "        name = name.replace('À', 'A')\n",
    "        name = name.replace('È', 'E')\n",
    "        name = name.replace('Ò', 'O')\n",
    "        name = name.replace('Ü', 'U')\n",
    "        name = name.replace('Ç', 'C')\n",
    "        name = name.replace('Ñ', 'N')\n",
    "        name = name.replace('?', ' ')\n",
    "\n",
    "        file = Path(path + cod + '_' + name + '.csv')\n",
    "        if file.is_file():\n",
    "            # Importar la serie preexistente\n",
    "            serie = pd.read_csv(file, encoding='latin1', na_values='NaN')\n",
    "            serie.fint = pd.to_datetime(serie.fint)\n",
    "            serie.set_index('fint', inplace=True, drop=True)\n",
    "            # Unir las nuevas observaciones\n",
    "            serie = serie.append(data_stn[df_stn.index > serie.index[-1]], verify_integrity=True)\n",
    "            # Exportar la serie\n",
    "            serie.to_csv(path + cod + '_' + name + '.csv', index=True, na_rep='NaN')\n",
    "            #del serie, df_stn#, raw_data\n",
    "        else:\n",
    "            data_stn.to_csv(path + cod + '_' + name + '.csv', index=True, na_rep='NaN')\n",
    "            #del data_stn#, raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Extraer datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'descripcion': 'exito', 'estado': 200, 'datos': 'https://opendata.aemet.es/opendata/sh/9c512188', 'metadatos': 'https://opendata.aemet.es/opendata/sh/55c2971b'}\n"
     ]
    }
   ],
   "source": [
    "if codigos[0] == 'todas':\n",
    "    # Extraer los datos\n",
    "    data_all = extract_hourly_all(ruta_API)\n",
    "    # Generar las series\n",
    "    generate_serie_all(data_all, ruta_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraer una única estación\n",
    "\n",
    "### 2.1 Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_data_stn(cod, name, API_path):\n",
    "    \"\"\"It extracts hourly climatic data from AEMET's API.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cod:      string. Station code. It must match a index value in the data frame 'stations'.\n",
    "    stations: data frame. Matrix of available stationes. Its index must be the station's code and it must have a field called\n",
    "              'ubi' which represents the station's name\n",
    "    API_path: string. The path where the API key text file is saved, including the name and file extension.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extraer de AEMET los datos del último día\n",
    "    # -----------------------------------------\n",
    "    # Carga la api key \n",
    "    api_key = open(API_path).read().rstrip()\n",
    "    querystring = {\"api_key\": api_key}\n",
    "\n",
    "    raw_data = []\n",
    "    \n",
    "    # Mostrar en pantalla la estación de trabajo\n",
    "    print()\n",
    "    print('Estación: ', cod, ' - ', name)\n",
    "    \n",
    "    # Obtenemos información de todas las estaciones disponibles\n",
    "    url = (\"https://opendata.aemet.es/opendata/api/observacion/convencional/datos/estacion//{station}\".format(\n",
    "            station=cod)\n",
    "          )\n",
    "    \n",
    "    iterate = True\n",
    "    \n",
    "    while iterate:\n",
    "        # Peticion de datos a la url indicada\n",
    "        r = requests.get(url, params=querystring, verify=False)\n",
    "        # Si no me deja hacer la conexión, la repito  \n",
    "        iterate = (r.status_code == requests.codes.too_many_requests)\n",
    "        print(r.json())\n",
    "        \n",
    "        # Chequeo si la petición ha ido bien    \n",
    "        if r.status_code == requests.codes.ok:\n",
    "            # Hago la petición para obtener los datos\n",
    "            data_url = r.json()['datos']\n",
    "            r_data = requests.get(data_url, params=querystring, verify=False)\n",
    "\n",
    "            # INCONSISTENCIA DE LA API:\n",
    "            # Cuando no encuentra datos en el rango seleccionado, la API devuelve\n",
    "            # que el status code es 200 (todo ok) y devuelve un json con el error\n",
    "            # cuando encuentra, no hay atributo estado            \n",
    "            try:\n",
    "                estado = r_data.json()['estado']\n",
    "            except:\n",
    "                estado = 200\n",
    "\n",
    "            # Si ha ido bien guardo los datos\n",
    "            if estado == requests.codes.ok:\n",
    "                #print(r_data.json())\n",
    "                raw_data.extend(r_data.json())\n",
    "            else:\n",
    "                print(r_data.json()['descripcion'])\n",
    "        else:\n",
    "            print(r.json()['descripcion'])\n",
    "\n",
    "        time.sleep(60/45)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_serie_stn(cod, name, raw_data, path):\n",
    "    \"\"\"It joins the past and present series of hourly climatic data. If there's not previous data, it simply creates a new\n",
    "    csv file with the hourly data from the last 24 h.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cod:      string. Station code.\n",
    "    name:     string. Station name.\n",
    "    raw_data: data frame. The result of function 'extract_hourly_data'.\n",
    "    path:     string. File path where the previous data is saved (or where the new series will be created).\n",
    "    \"\"\"\n",
    "    # Reorganizar la serie si hay observaciones o hay datos previos\n",
    "    # -------------------------------------------------------------     \n",
    "    # Asegurar que el nuevo data frame tiene 24 filas (con NaN si no hay obervación)\n",
    "    start = datetime.datetime.now().replace(microsecond=0,second=0,minute=0) - datetime.timedelta(hours=24)\n",
    "    horas = pd.DatetimeIndex(freq='H', start=start, periods=24)\n",
    "    columns = ['idema', 'ubi', 'lat', 'lon', 'alt', 'prec', 'ta', 'tamax', 'tamin', 'vv', 'dv',\n",
    "                              'vmax', 'dmax', 'hr']\n",
    "    df2 = pd.DataFrame(index=horas, columns=columns)\n",
    "    df2.index.name = 'hora'\n",
    "    # Si hay datos disponibles de las últimas horas introducir dichos datos\n",
    "    if len(raw_data) > 0:\n",
    "        # Crear una lista de data frames con los datos de cada hora\n",
    "        dfs = []\n",
    "        for i in np.arange(len(raw_data)):\n",
    "            dfs.append(pd.DataFrame(raw_data[i], index=[i]))\n",
    "\n",
    "        # Combinar los data frames de la lista en un unico data frame\n",
    "        df = pd.concat(dfs)\n",
    "        df.fint = pd.to_datetime(df.fint, yearfirst=True)\n",
    "        df.set_index('fint', inplace=True)\n",
    "        df2.loc[df.index, :] = df\n",
    "        #del df, dfs\n",
    "    \n",
    "    file = Path(path + cod + '_' + name + '.csv')\n",
    "    if file.is_file():\n",
    "        # Importar la serie preexistente\n",
    "        serie = pd.read_csv(file, encoding='latin1', na_values='NaN')\n",
    "        serie.hora = pd.to_datetime(serie.hora)\n",
    "        serie.set_index('hora', inplace=True, drop=True)\n",
    "        # Unir las nuevas observaciones\n",
    "        serie = serie.append(df2[df2.index > serie.index[-1]], verify_integrity=True)\n",
    "        # Exportar la serie\n",
    "        serie.to_csv(path + cod + '_' + name + '.csv',\n",
    "               index=True, na_rep='NaN')\n",
    "        del serie, df2, raw_data\n",
    "    else:\n",
    "        if len(raw_data) > 0:\n",
    "            df2.to_csv(path + cod + '_' + name + '.csv',\n",
    "               index=True, na_rep='NaN')\n",
    "            del df2, raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Aplicarlo a todas las estaciones en bucle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if codigos[0] != 'todas':\n",
    "    for stn in codigos:\n",
    "        # Nombre de la estación\n",
    "        nombre = estaciones.loc[stn, ubi]\n",
    "        # Extraer datos\n",
    "        data_stn = extract_data_stn(stn, nombre, ruta_API)\n",
    "        # Generar la serie\n",
    "        generate_serie_stn(stn, nombre, data_stn, ruta_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'schedule'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-aaf3421c5d55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'schedule'"
     ]
    }
   ],
   "source": [
    "import schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
