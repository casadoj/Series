{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hourly_all(API_path, series_path):\n",
    "    \"\"\"It extracts hourly climatic data from AEMET's API for all the stations available during the last 24 h.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    API_path:     string. Path where the API key text file is saved, including the name and file extension.\n",
    "    series_path:  string. File path where the previous data is saved (or where the new series will be created). \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extraer de AEMET los datos del último día\n",
    "    # -----------------------------------------\n",
    "     # Carga la api key \n",
    "    api_key = open(API_path).read().rstrip()\n",
    "    querystring = {\"api_key\": api_key}\n",
    "\n",
    "    # Obtenemos información de todas las estaciones disponibles\n",
    "    url = \"https://opendata.aemet.es/opendata/api/observacion/convencional/todas\"\n",
    "    \n",
    "    iterate = True\n",
    "\n",
    "    while iterate:\n",
    "\n",
    "        # Peticion de datos a la url indicada\n",
    "        r = requests.get(url, params=querystring, verify=False)\n",
    "        # Si no me deja hacer la conexión, la repito  \n",
    "        iterate = (r.status_code == requests.codes.too_many_requests)\n",
    "        print(r.json())\n",
    "        \n",
    "\n",
    "        # Chequeo si la petición ha ido bien    \n",
    "        if r.status_code == requests.codes.ok:\n",
    "            # Hago la petición para obtener los datos\n",
    "            data_url = r.json()['datos']\n",
    "            r_data = requests.get(data_url, params=querystring, verify=False)\n",
    "\n",
    "            # INCONSISTENCIA DE LA API:\n",
    "            # Cuando no encuentra datos en el rango seleccionado, la API devuelve\n",
    "            # que el status code es 200 (todo ok) y devuelve un json con el error\n",
    "            # cuando encuentra, no hay atributo estado            \n",
    "            try:\n",
    "                estado = r_data.json()['estado']\n",
    "            except:\n",
    "                estado = 200\n",
    "\n",
    "            # Si ha ido bien, guardo los datos\n",
    "            if estado == requests.codes.ok:\n",
    "                #print(r_data.json())\n",
    "                data = (r_data.json())\n",
    "                # Crear una lista con cada una de las entradas de los datos brutos convertidos en data frames\n",
    "                ls = []\n",
    "                for i in np.arange(len(data)):\n",
    "                    ls.append(pd.DataFrame(data[i], index=[i]))\n",
    "                # Combinar los data frames de la lista en un unico data frame y definir la hora como índice\n",
    "                data = pd.concat(ls, axis=0, sort=True)\n",
    "                data.fint = pd.to_datetime(data.fint, yearfirst=True)\n",
    "                data.set_index('fint', inplace=True)\n",
    "                # Reorganizar algunas columnas\n",
    "                col = list(data.columns)\n",
    "                col.remove('alt')\n",
    "                col.remove('idema')\n",
    "                col.remove('lat')\n",
    "                col.remove('lon')\n",
    "                col.remove('ubi')\n",
    "                col = ['idema', 'ubi', 'lat', 'lon', 'alt'] + col\n",
    "                data = data[col]\n",
    "            else:\n",
    "                print(r_data.json()['descripcion'])\n",
    "        else:\n",
    "            print(r.json()['descripcion'])\n",
    "\n",
    "        time.sleep(60/45)\n",
    "\n",
    "    # Nombre de las estaciones\n",
    "    estaciones = data.idema.unique()\n",
    "    \n",
    "    for cod in estaciones:\n",
    "        print('Estación {0:<5}'.format(cod), end='\\r')\n",
    "        # Extraer las filas correspondientes a una estación\n",
    "        data_stn = data.loc[data.idema == cod, :]\n",
    "        name = data_stn.ubi[0]\n",
    "        # Corregir caracteres conflictivos en el nombre de las estaciones\n",
    "        old = ['/', '\"', 'Á', 'É', 'Í', 'Ó', 'Ú', 'À', 'È', 'Ò', 'Ü', 'Ç', 'Ñ', '?']\n",
    "        new = ['-', '', 'A', 'E', 'I', 'O', 'U', 'A', 'E', 'O', 'U', 'C', 'N', ' ']\n",
    "        for o, n in zip(old, new):\n",
    "            name = name.replace(o, n)       \n",
    "\n",
    "        file = Path(series_path + cod + '.csv')\n",
    "        if file.is_file():\n",
    "            # Importar la serie preexistente\n",
    "            serie = pd.read_csv(file, parse_dates=['fint'], dayfirst=False, index_col=0,\n",
    "                                encoding='latin1', na_values='')\n",
    "            # Unir las nuevas observaciones\n",
    "            #serie = serie.append(data_stn[df_stn.index > serie.index[-1]], verify_integrity=True)\n",
    "            serie = pd.concat((serie, data_stn), axis=0, join='outer', sort=True).drop_duplicates()\n",
    "            # Exportar la serie\n",
    "            serie.to_csv(series_path + cod + '.csv', index=True, na_rep='')\n",
    "        else:\n",
    "            data_stn.to_csv(series_path + cod + '.csv', index=True, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_stn(cod, name, API_path):\n",
    "    \"\"\"It extracts hourly climatic data from AEMET's API.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cod:      string. Station code. It must match a index value in the data frame 'stations'.\n",
    "    stations: data frame. Matrix of available stationes. Its index must be the station's code and it must have a field called\n",
    "              'ubi' which represents the station's name\n",
    "    API_path: string. The path where the API key text file is saved, including the name and file extension.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extraer de AEMET los datos del último día\n",
    "    # -----------------------------------------\n",
    "    # Carga la api key \n",
    "    api_key = open(API_path).read().rstrip()\n",
    "    querystring = {\"api_key\": api_key}\n",
    "\n",
    "    raw_data = []\n",
    "    \n",
    "    # Mostrar en pantalla la estación de trabajo\n",
    "    print()\n",
    "    print('Estación: ', cod, ' - ', name)\n",
    "    \n",
    "    # Obtenemos información de todas las estaciones disponibles\n",
    "    url = (\"https://opendata.aemet.es/opendata/api/observacion/convencional/datos/estacion//{station}\".format(\n",
    "            station=cod)\n",
    "          )\n",
    "    \n",
    "    iterate = True\n",
    "    \n",
    "    while iterate:\n",
    "        # Peticion de datos a la url indicada\n",
    "        r = requests.get(url, params=querystring, verify=False)\n",
    "        # Si no me deja hacer la conexión, la repito  \n",
    "        iterate = (r.status_code == requests.codes.too_many_requests)\n",
    "        print(r.json())\n",
    "        \n",
    "        # Chequeo si la petición ha ido bien    \n",
    "        if r.status_code == requests.codes.ok:\n",
    "            # Hago la petición para obtener los datos\n",
    "            data_url = r.json()['datos']\n",
    "            r_data = requests.get(data_url, params=querystring, verify=False)\n",
    "\n",
    "            # INCONSISTENCIA DE LA API:\n",
    "            # Cuando no encuentra datos en el rango seleccionado, la API devuelve\n",
    "            # que el status code es 200 (todo ok) y devuelve un json con el error\n",
    "            # cuando encuentra, no hay atributo estado            \n",
    "            try:\n",
    "                estado = r_data.json()['estado']\n",
    "            except:\n",
    "                estado = 200\n",
    "\n",
    "            # Si ha ido bien guardo los datos\n",
    "            if estado == requests.codes.ok:\n",
    "                #print(r_data.json())\n",
    "                raw_data.extend(r_data.json())\n",
    "            else:\n",
    "                print(r_data.json()['descripcion'])\n",
    "        else:\n",
    "            print(r.json()['descripcion'])\n",
    "\n",
    "        time.sleep(60/45)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_serie_all(data, path):\n",
    "    \"\"\"It joins the past and present series of hourly climatic data. If there's not previous data, it simply creates a new\n",
    "    csv file with the hourly data from the last 24 h.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data:     data frame. The result of function 'extract_hourly_data'.\n",
    "    path:     string. File path where the previous data is saved (or where the new series will be created).\n",
    "    \"\"\"\n",
    "    # Nombre de las estaciones\n",
    "    estaciones = data.idema.unique()\n",
    "    \n",
    "    for i in range(len(estaciones)):\n",
    "        # Extraer las filas correspondientes a una estación\n",
    "        data_stn = data.loc[data.idema == estaciones[i], :]\n",
    "        cod = data_stn.idema[0]\n",
    "        name = data_stn.ubi[0]\n",
    "        # Corregir caracteres conflictivos en el nombre de las estaciones\n",
    "        name = name.replace('/', '-')\n",
    "        name = name.replace('\"', '')\n",
    "        name = name.replace('Á', 'A')\n",
    "        name = name.replace('É', 'E')\n",
    "        name = name.replace('Í', 'I')\n",
    "        name = name.replace('Ó', 'O')\n",
    "        name = name.replace('Ú', 'U')\n",
    "        name = name.replace('À', 'A')\n",
    "        name = name.replace('È', 'E')\n",
    "        name = name.replace('Ò', 'O')\n",
    "        name = name.replace('Ü', 'U')\n",
    "        name = name.replace('Ç', 'C')\n",
    "        name = name.replace('Ñ', 'N')\n",
    "        name = name.replace('?', ' ')\n",
    "\n",
    "        file = Path(path + cod + '_' + name + '.csv')\n",
    "        if file.is_file():\n",
    "            # Importar la serie preexistente\n",
    "            serie = pd.read_csv(file, encoding='latin1', na_values='NaN')\n",
    "            serie.fint = pd.to_datetime(serie.fint)\n",
    "            serie.set_index('fint', inplace=True, drop=True)\n",
    "            # Unir las nuevas observaciones\n",
    "            serie = serie.append(data_stn[df_stn.index > serie.index[-1]], verify_integrity=True)\n",
    "            # Exportar la serie\n",
    "            serie.to_csv(path + cod + '_' + name + '.csv', index=True, na_rep='NaN')\n",
    "            #del serie, df_stn#, raw_data\n",
    "        else:\n",
    "            data_stn.to_csv(path + cod + '_' + name + '.csv', index=True, na_rep='NaN')\n",
    "            #del data_stn#, raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
